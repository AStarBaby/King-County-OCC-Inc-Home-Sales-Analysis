{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 0
   },
   "source": [
    "# Capstone Regression Project\n",
    "### Scharmaine Chappell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "index": 2
   },
   "source": [
    "## Business Understanding\n",
    "My stakeholder, One Call Concepts, Inc. is wanting to prepare for the next busy season. One Call Concepts, known by many different names, including DigSafe, is ran as Washington Utility Notification Center in Washington. (http://www.callbeforeyoudig.org/washington/faq.asp#q1) They are the middle man when a contractor, or homeowner, or anyone, wants to dig and the underground facility locating companies. OCC maintains databases of said underground facilities and uses that information to notify owners of these facilities locators of a potential dig site. When rules are followed, this allows for safer digging, lessening chances of a potential damaged line.\n",
    "\n",
    "Working with King County, Washington, OCC has learned that the county is looking to provide incentives to new homeowners of these properties. This predicting model will allow the county to discern what type or amount of incentive to provide should they choose one of these areas. Encouraging economic growth and a more inviting, natural habitation throughout King County. Which in turn should increase interest in their county from tourists and possible new residents(constituents). Therefore, in order to encourage their working relationship, OCC would like to be able to predict the final sale prices of properties currently in areas with no view, needing beautification. Leading to increased awareness campaigns in those areas by OCC, as well as building current and new relationships with owners of underground facilities in those areas.\n",
    "\n",
    "We will begin narrowing the variables by view. We will then remove the price outliers. From the cleaned dataset we will start with the square footage of the lot, total living area and the area above ground."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 2
   },
   "source": [
    "## Data Understanding\n",
    "What we'll do is use data gathered on the county from 2021 - 2022 home sales data for King County Washington. https://data.kingcounty.gov/ .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 2
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\", palette=\"rocket\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 3,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 8
   },
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review label, types and for null values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for which is carrying the most weight, mean, of the numerical catagories\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that id is the heaviest, then price, sqft_lot, sqft_living, yr_built, sqft_above. ID is the heaviest, but is not relevant for our problem, so we will drop that column first. Price will be our target. We want to know what percentage of the lot the total living space takes up. And how that takes effects the final price of the property. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for multicollinearity prior to clean up\n",
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price.corr(df.sqft_lot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price.corr(df.sqft_living)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqft_above is the area of the home that is above ground\n",
    "df.price.corr(df.sqft_above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our heat_source and sewer_system have a negligent amount of null values in respect to the size of the size of the dataset with 32 and 14 out of 30155 entries. Therefore, we will remove these rows from our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis = 0, inplace = True)\n",
    "#confirm null values have been removed\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shape of the data verify as well\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the date column label to date sold to clarify what the information is representing\n",
    "sold = {\"date\" : \"datesold\"}\n",
    "df.rename(columns=sold, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing our datesold column from type object to type datetime \n",
    "df.datesold = df.datesold.apply(lambda x: pd.to_datetime(x, yearfirst=True))\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating a new colume, 'age', from the 'yr_renovated' and 'yr_built' columns\n",
    "df[\"age\"] = np.where(df[\"yr_renovated\"] != 0, df.datesold.apply(lambda x:x.year) - df[\"yr_renovated\"],\n",
    "df[\"datesold\"].apply(lambda x:x.year) - df[\"yr_built\"])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing current irrelevant columns\n",
    "df.drop(axis = 1, labels = {'datesold','id', 'yr_renovated', 'yr_built','lat', 'long'}, inplace = True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review the address data to determine how to create a new zipcode column\n",
    "df.address.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.address[30111][-20:-15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.address[30111].split(',')[2][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"zips\"] = df.address.apply(lambda x: x[-20:-15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sampling the new 'zips' column to check format\n",
    "df.zips.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we've separated the zip codes, we can remove the 'address' column\n",
    "df.drop(axis = 1, labels = 'address', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify column removed\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 35
   },
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review updated dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Checking our 'view' column we see that 'NONE' is the most frequent response\n",
    "#at this point we also meet our number of rows, entries, requirements\n",
    "df.view.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new dataframe with only numerical values\n",
    "df_num = df[[\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\"floors\",\"sqft_above\",\"sqft_basement\",\"sqft_garage\",\"sqft_patio\",\"age\"]]\n",
    "df_num.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using just the initial numerical values to create baseline model\n",
    "pred = df_num\n",
    "target = df.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning X and y values\n",
    "X = pred\n",
    "y = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseline = sm.OLS(y, sm.add_constant(X))\n",
    "results = baseline.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our R-squared is less 40.9% using just the current numerical values as predictors and 'price' as our target. Our F-statistic and P-values are sbelow .5 as well.\n",
    "\n",
    "### We will create some dummy variables for our catagorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking for multicollinearity\n",
    "sns.heatmap(X.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that 'sqft_living' and 'sqft_above' (how many square feet of living space is above ground) are most correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sqft_above.corr(df.sqft_living)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewing data for catagorical columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reviewing a sample of the types of values in the catagorical values\n",
    "df[[\"waterfront\", \"greenbelt\", \"nuisance\", \"view\", \"condition\", \"grade\", \"heat_source\", \"sewer_system\", \"zips\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating a dataframe catagorical dummy values, excluding zipcodes as we do not need them at this time \n",
    "#and they will highly skew our results\n",
    "cats= [\"waterfront\", \"greenbelt\", \"nuisance\", \"view\", \"condition\", \"grade\", \"heat_source\", \"sewer_system\"]\n",
    "df_dummy= pd.get_dummies(data = df, columns = cats, drop_first=True)\n",
    "df_dummy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#removing spaces from column names and replacing with '_'\n",
    "df_dummy.columns = df_dummy.columns.str.replace(' ', '_')\n",
    "df_dummy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#creating dataframe with only rows where 'view_NONE' is True\n",
    "nview_df = df_dummy[df_dummy.view_NONE == 1]\n",
    "nview_df.view_NONE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirming it's the entire df\n",
    "nview_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#remove currently irrelevant 'view' dummy values that are NaN\n",
    "\n",
    "nview_df.drop(axis = 1, labels = {'view_EXCELLENT','view_FAIR', 'view_GOOD', 'view_NONE'}, inplace = True)\n",
    "#verify columns removed\n",
    "nview_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking size of new dataframe\n",
    "nview_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#determining upper and lower price of these no view property\n",
    "nview_df.price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting 'price' values of range_df\n",
    "sns.boxplot(data = nview_df, x= 'price', color = \"blue\", palette = \"rocket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#removing unwanted outliers\n",
    "min_reach, max_reach = nview_df.price.quantile([0.05, 0.95])\n",
    "min_reach, max_reach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nview_df[nview_df.price > max_reach]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nview_df[nview_df.price < min_reach]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating new dataframe to represent view_NONE values only within our new price range\n",
    "range_df = nview_df[(nview_df.price < max_reach) & (nview_df.price > min_reach)]\n",
    "range_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "range_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting 'price' values of range_df update\n",
    "sns.boxplot(data = range_df, x= 'price', color = \"blue\", palette = \"rocket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#reviewing for values of 0, removing these values and zips at this time\n",
    "range_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create new df removing currently irrelevant dummy, 'zips' columns\n",
    "clean_df = range_df.drop(axis = 1, labels = {'zips','grade_12_Luxury', 'grade_13_Mansion', 'grade_2_Substandard'})\n",
    "#verify columns removed\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_2 = clean_df.drop(labels = ['price'], axis = 1)\n",
    "target_2 = clean_df.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = preds_2\n",
    "y_2 = target_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_2 = sm.OLS(y_2, sm.add_constant(X_2))\n",
    "results_2 = model_2.fit()\n",
    "print(results_2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our R-squared using is now 42.9%, F-statistic is below 0. We have mostly P-statistics above .5, but some are above this max. Looking at our values for 'sqft_lot', 'sqft_living' and 'sqft_above',  their P-statistics are below 0. So, we can move forward using those as our predictor values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check our current data's correlations with price\n",
    "clean_df.price.corr(df.sqft_lot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_df.price.corr(df.sqft_living)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.price.corr(df.sqft_above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_2.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 67
   },
   "source": [
    "## Regression Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model represented is:\n",
    "### Price = 812,600 + 0.3841(sqft_lot) + 96.8246(sqft_above) + 94.7702(sqft_living)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping in mind we are reviewing data only pertaining to original entries listed as view_NONE between the 95th and 5th percentiles of price:\n",
    "\n",
    "#### Overall, this model is statistically significant with a t-statistic p-value and overall F_p-value still below 5%. \n",
    "\n",
    "##### This shows our sqft_lot, sqft_above and sqft_living parameters each significantly impact price\n",
    "##### For each sf of increase in sqft_Lot we only gain .38 units in Price, even though it's p-value is still below 5%.\n",
    "##### This shows us that sqft_lot is not a good fit for this linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This will model our chosen predictors alone without the effects of the other two\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "sm.graphics.plot_partregress_grid(results_2, exog_idx=[\"sqft_lot\", \"sqft_above\", \"sqft_living\"], fig=fig)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 86
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In conclusion, we can see that our initially chosen parameters sqft_lot, sqft_above, and sqft_living have some significance in determining final home selling price. With that said sqft_lot for this particular set of data, does not seem to add much to the price. However, their may be other factors that we may want to consider. We need to discern if the view values of 'NONE' are accurate, really NaN values, or are misleading in some other fashion. Other considerations may be the factors of condition and grade as they have p-values below 5% as well, we also see a larger stastistically significant impact on sales price of these homes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you,\n",
    "### Scharmaine Chappell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
